{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore temporal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group by 'child_sha' to collect all files modified in a commit\n",
    "commits = []\n",
    "for child_sha, group in df.groupby(\"child_sha\"):\n",
    "    commit_files = list(group[\"new_file\"].unique())\n",
    "    commits.append({\"commit_id\": child_sha, \"files\": commit_files})\n",
    "\n",
    "def jaccard_similarity(files_t, files_t1):\n",
    "    set_t = set(files_t)\n",
    "    set_t1 = set(files_t1)\n",
    "    if not set_t and not set_t1:\n",
    "        return 1  # Both empty, max similarity\n",
    "    return len(set_t & set_t1) / len(set_t | set_t1)\n",
    "\n",
    "# Compute Jaccard similarity between consecutive commits\n",
    "similarities = []\n",
    "for i in range(len(commits) - 1):\n",
    "    sim = jaccard_similarity(commits[i]['files'], commits[i+1]['files'])\n",
    "    similarities.append(sim)\n",
    "\n",
    "print(\"Average Jaccard Similarity:\", np.mean(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import itertools\n",
    "# import random\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Ignore renames\n",
    "    df = df[df[\"old_file\"] == df[\"new_file\"]]\n",
    "    # Group by commit\n",
    "    commits = df.groupby(\"child_sha\").agg({\n",
    "        \"old_file\": lambda x: list(x),\n",
    "        \"when\": \"first\"\n",
    "    }).reset_index()\n",
    "    # Sort by timestamp\n",
    "    commits = commits.sort_values(by=\"when\")\n",
    "    return commits\n",
    "\n",
    "# Train-test split\n",
    "def train_test_split(commits, test_size=0.3):\n",
    "    split_idx = int(len(commits) * (1 - test_size))\n",
    "    train_commits = commits.iloc[:split_idx]\n",
    "    test_commits = commits.iloc[split_idx:]\n",
    "    return train_commits, test_commits\n",
    "\n",
    "# Baseline 1: Co-occurrence-based model\n",
    "class CoOccurrenceModel:\n",
    "    def __init__(self):\n",
    "        self.co_occurrence = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def train(self, train_commits):\n",
    "        for files in train_commits[\"old_file\"]:\n",
    "            for i in range(len(files)):\n",
    "                for j in range(i + 1, len(files)):\n",
    "                    self.co_occurrence[files[i]][files[j]] += 1\n",
    "                    self.co_occurrence[files[j]][files[i]] += 1\n",
    "\n",
    "    def predict(self, input_files, top_k=5):\n",
    "        scores = defaultdict(int)\n",
    "        for file in input_files:\n",
    "            if file in self.co_occurrence:\n",
    "                for neighbor, count in self.co_occurrence[file].items():\n",
    "                    scores[neighbor] += count\n",
    "        # Remove input files from predictions\n",
    "        for file in input_files:\n",
    "            scores.pop(file, None)\n",
    "        # Sort by score\n",
    "        sorted_files = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n",
    "        return sorted_files[:top_k]\n",
    "    \n",
    "def evaluate_model(model, test_commits, top_k=10):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for _, row in test_commits.iterrows():\n",
    "        files = row[\"old_file\"]\n",
    "        if len(files) < 2:\n",
    "            continue\n",
    "        # Hold out one file as input, rest as ground truth\n",
    "        for i in range(len(files)):\n",
    "            input_files = [files[i]]\n",
    "            true_files = files[:i] + files[i+1:]\n",
    "            pred_files = model.predict(input_files, top_k=len(true_files))\n",
    "            # print(pred_files)\n",
    "            # y_true.append([1 if f in true_files else 0 for f in pred_files])\n",
    "            # y_pred.append([1] * len(pred_files))\n",
    "\n",
    "            y_true.append([1 if f in true_files else 0 for f in all_files])\n",
    "            y_pred.append([1 if f in pred_files else 0 for f in all_files])\n",
    "    # Flatten lists\n",
    "    y_true = [item for sublist in y_true for item in sublist]\n",
    "    y_pred = [item for sublist in y_pred for item in sublist]\n",
    "    # print(len(y_true),y_true[:200])\n",
    "    # print(y_pred[:200])\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset has 7179 rows.\n",
      "1931310 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Co-occurrence Model - Precision: 0.1394, Recall: 0.1230\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../ffmpeg-master-none.csv\")\n",
    "\n",
    "# Filter out files that appear in less than 3 commits\n",
    "file_counts = df[\"new_file\"].value_counts()\n",
    "filtered_files = file_counts[file_counts >= 3].index\n",
    "\n",
    "df_filtered = df[df['new_file'].isin(filtered_files)].copy()\n",
    "print(f\"Filtered dataset has {len(df_filtered)} rows.\")\n",
    "df = df_filtered\n",
    "\n",
    "all_files = df[\"new_file\"].unique()\n",
    "\n",
    "commits = preprocess_data(df)\n",
    "train_commits, test_commits = train_test_split(commits)\n",
    "\n",
    "# Train and evaluate co-occurrence model\n",
    "co_occurrence_model = CoOccurrenceModel()\n",
    "co_occurrence_model.train(train_commits)\n",
    "co_precision, co_recall = evaluate_model(co_occurrence_model, test_commits)\n",
    "print(f\"Co-occurrence Model - Precision: {co_precision:.4f}, Recall: {co_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
